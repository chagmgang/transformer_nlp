{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitpytorchcondaed36d89d6ca44e38823f2dd584949822",
   "display_name": "Python 3.6.10 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import json\n",
    "import torch\n",
    "import tokenization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformer.model import Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'kor_vocab_length': 50000,\n 'eng_vocab_length': 28998,\n 'd_model': 768,\n 'd_ff': 2048,\n 'd_k': 64,\n 'd_v': 64,\n 'num_layers': 12,\n 'num_heads': 8,\n 'start_word': '[SOS]',\n 'end_word': '[EOS]',\n 'sep_word': '[SEP]',\n 'cls_word': '[CLS]',\n 'pad_word': '[PAD]',\n 'mask_word': '[MASK]'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read configuration file\n",
    "config = json.load(open('config.json'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From /Users/chageumgang/Desktop/transformer_nlp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\n['I', 'love', 'you']\n['나는', '너를', '사랑', '##한다']\n"
    }
   ],
   "source": [
    "src_tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='vocab/kor_vocab.txt', do_lower_case=False)\n",
    "tgt_tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='vocab/eng_vocab.txt', do_lower_case=False)\n",
    "print(tgt_tokenizer.tokenize('I love you'))\n",
    "print(src_tokenizer.tokenize('나는 너를 사랑한다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[47488, 47728,  1722,  2272, 21191,  2540, 13330, 13201,  3807,  7195,\n         47488, 47459, 47492, 27321, 22775, 21385,  9246,  4037, 47558, 47467,\n          1634, 33068,  7923, 48117, 22183, 47440,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [25413, 30745, 47471, 24155, 47774,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n        [ 1612,   213, 46686,     3,  1914,  1253, 47491,  7942,  9246, 47555,\n         15713, 10552, 47668,  1239, 26620, 38688, 47461, 11548,   151,     3,\n         47440,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])\ntensor([[    1,  5907, 13068,  1160,   114,  1112,   172, 25889,  4050,  1117,\n          3645,  1130,  1108,  2543,  2714,  2803,  1109,  1105,  5907,   121,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0],\n        [    1,  2093,  1130,  1252,  1122,   172,  1394,  3087,   138,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0],\n        [    1,  1132,  2945,  1431,  3768,  1272, 26503,  1123,  1105,  8882,\n          1107,  2122,  1142,  1123,  1105,  2046,   121,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0]])\ntensor([[ 5907, 13068,  1160,   114,  1112,   172, 25889,  4050,  1117,  3645,\n          1130,  1108,  2543,  2714,  2803,  1109,  1105,  5907,   121,     2,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0],\n        [ 2093,  1130,  1252,  1122,   172,  1394,  3087,   138,     2,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0],\n        [ 1132,  2945,  1431,  3768,  1272, 26503,  1123,  1105,  8882,  1107,\n          2122,  1142,  1123,  1105,  2046,   121,     2,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0]])\n"
    }
   ],
   "source": [
    "# 'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다. -> Bible Coloring' is a coloring application that allows you to experience beautiful stories in the Bible.\n",
    "# 씨티은행에서 일하세요? -> Do you work at a City bank?\n",
    "# 11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다. -> In Chapter 11 Jesus called Lazarus from the tomb and raised him from the dead.\n",
    "\n",
    "src_text = [\n",
    "    '\\'Bible Coloring\\'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.',\n",
    "    '씨티은행에서 일하세요?', '11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.']\n",
    "tgt_text = [\n",
    "    'Bible Coloring\\' is a coloring application that allows you to experience beautiful stories in the Bible.',\n",
    "    'Do you work at a City bank?', 'In Chapter 11 Jesus called Lazarus from the tomb and raised him from the dead.']\n",
    "\n",
    "enc_length = 40\n",
    "tgt_length = 40\n",
    "\n",
    "enc_inputs = []\n",
    "for text in src_text:\n",
    "    tokens = src_tokenizer.tokenize(text)\n",
    "    tokens = tokens[:enc_length]\n",
    "    while len(tokens) < enc_length:\n",
    "        tokens.append(config['pad_word'])\n",
    "    enc_input = src_tokenizer.convert_tokens_to_ids(tokens)\n",
    "    enc_inputs.append(enc_input)\n",
    "\n",
    "dec_inputs, dec_outputs = [], []\n",
    "for text in tgt_text:\n",
    "    tokens = [config['start_word']]\n",
    "    tokens.extend(tgt_tokenizer.tokenize(text)[:tgt_length])\n",
    "    tokens.append(config['end_word'])\n",
    "    \n",
    "    dec_input = tokens[:-1]\n",
    "    dec_output = tokens[1:]\n",
    "\n",
    "    while len(dec_input) < tgt_length + 1:\n",
    "        dec_input.append(config['pad_word'])\n",
    "    while len(dec_output) < tgt_length + 1:\n",
    "        dec_output.append(config['pad_word'])\n",
    "\n",
    "    dec_input = tgt_tokenizer.convert_tokens_to_ids(dec_input)\n",
    "    dec_output = tgt_tokenizer.convert_tokens_to_ids(dec_output)\n",
    "\n",
    "    dec_inputs.append(dec_input)\n",
    "    dec_outputs.append(dec_output)\n",
    "\n",
    "enc_inputs_tensor = torch.as_tensor(enc_inputs, dtype=torch.long).to(device)\n",
    "dec_inputs_tensor = torch.as_tensor(dec_inputs, dtype=torch.long).to(device)\n",
    "dec_outputs_tensor = torch.as_tensor(dec_outputs, dtype=torch.long).to(device)\n",
    "\n",
    "print(enc_inputs_tensor)\n",
    "print(dec_inputs_tensor)\n",
    "print(dec_outputs_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": " (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (5): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (6): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (7): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (8): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (9): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (10): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (11): EncoderLayer(\n        (enc_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (decoder): Decoder(\n    (tgt_emb): Embedding(28998, 768)\n    (pos_emb): PositionalEncoding(\n      (dropout): Dropout(p=0, inplace=False)\n    )\n    (layers): ModuleList(\n      (0): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (1): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (2): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (3): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (4): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (5): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (6): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (7): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (8): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (9): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (10): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (11): DecoderLayer(\n        (dec_self_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (dec_enc_attn): MultiHeadAttention(\n          (WQ): Linear(in_features=768, out_features=512, bias=True)\n          (WK): Linear(in_features=768, out_features=512, bias=True)\n          (WV): Linear(in_features=768, out_features=512, bias=True)\n          (linear): Linear(in_features=512, out_features=768, bias=True)\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n        (pos_ffn): PoswiseFeedForwardNet(\n          (l1): Linear(in_features=768, out_features=2048, bias=True)\n          (l2): Linear(in_features=2048, out_features=768, bias=True)\n          (relu): GELU()\n          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n    )\n  )\n  (projection): Linear(in_features=768, out_features=28998, bias=False)\n)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## configure model, optimizer, criterion\n",
    "src_pad_index = src_tokenizer.convert_tokens_to_ids([config['pad_word']])[0]\n",
    "tgt_pad_index = tgt_tokenizer.convert_tokens_to_ids([config['pad_word']])[0]\n",
    "\n",
    "transformer = Translation(\n",
    "    src_vocab_size=config['kor_vocab_length'],\n",
    "    tgt_vocab_size=config['eng_vocab_length'],\n",
    "    d_model=config['d_model'], d_ff=config['d_ff'],\n",
    "    d_k=config['d_k'], d_v=config['d_v'], n_heads=config['num_heads'], \n",
    "    n_layers=config['num_layers'], src_pad_index=src_pad_index,\n",
    "    tgt_pad_index=src_pad_index, device=device).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=5e-5)\n",
    "transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 4.10388708114624\n1 3.678687334060669\n2 3.5360214710235596\n3 3.406212091445923\n4 3.2002038955688477\n5 3.103100061416626\n6 2.9335062503814697\n7 2.7363855838775635\n8 2.6097939014434814\n9 2.4988210201263428\n10 2.375528335571289\n11 2.242288589477539\n12 2.1073808670043945\n13 1.9714640378952026\n14 1.8304811716079712\n15 1.6902967691421509\n16 1.5418894290924072\n17 1.386192798614502\n18 1.2296375036239624\n19 1.0829826593399048\n20 0.9372422099113464\n21 0.7987121343612671\n22 0.6721315383911133\n23 0.5591924786567688\n24 0.4610476791858673\n25 0.3770865797996521\n26 0.30839186906814575\n27 0.2527344822883606\n28 0.20801861584186554\n29 0.17255273461341858\n30 0.144961878657341\n31 0.12296938896179199\n32 0.1052861139178276\n33 0.09093289077281952\n34 0.0792354866862297\n35 0.06968825310468674\n36 0.06177357956767082\n37 0.055134180933237076\n38 0.04958122596144676\n39 0.04493553936481476\n40 0.04098932817578316\n41 0.037585072219371796\n42 0.034641340374946594\n43 0.03210744261741638\n44 0.029924551025032997\n45 0.02802356891334057\n46 0.026347827166318893\n47 0.024858908727765083\n48 0.023531122133135796\n49 0.022343412041664124\n50 0.02127603441476822\n51 0.020312028005719185\n52 0.01943969912827015\n53 0.018651574850082397\n54 0.01793946698307991\n55 0.017294850200414658\n56 0.0167098306119442\n57 0.016174770891666412\n58 0.015683649107813835\n59 0.015231526456773281\n60 0.01481384877115488\n61 0.01442689262330532\n62 0.014067346230149269\n63 0.013731589540839195\n64 0.013417464680969715\n65 0.013123616576194763\n66 0.012848717160522938\n67 0.012591107748448849\n68 0.012349680066108704\n69 0.012123084627091885\n70 0.01190904714167118\n71 0.011706547811627388\n72 0.011514350771903992\n73 0.01133180782198906\n74 0.011158043518662453\n75 0.01099271047860384\n76 0.010834923014044762\n77 0.010683986358344555\n78 0.010539337992668152\n79 0.01040076743811369\n80 0.0102675287052989\n81 0.010139092803001404\n82 0.01001586951315403\n83 0.009896859526634216\n84 0.009781969711184502\n85 0.009671003557741642\n86 0.00956354383379221\n87 0.00945944245904684\n88 0.009358552284538746\n89 0.009260809980332851\n90 0.009165854193270206\n91 0.009073515422642231\n92 0.008983748033642769\n93 0.008896546438336372\n94 0.008811365813016891\n95 0.008728163316845894\n96 0.008647178299725056\n97 0.008567962795495987\n98 0.008490659296512604\n99 0.008415193296968937\n"
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns = transformer(enc_inputs_tensor, dec_inputs_tensor)\n",
    "    loss = criterion(\n",
    "        dec_logits.view(-1, dec_logits.size(-1)),\n",
    "        dec_outputs_tensor.contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.\norig_text    : 'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.\npredict_text : [SOS] Bible Coloring ' is a coloring application that allows you to experience beautiful stories in the Bible . [EOS]\n-----------------\n씨티은행에서 일하세요?\norig_text    : 씨티은행에서 일하세요?\npredict_text : [SOS] Do you work at a City bank ? [EOS]\n-----------------\n11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.\norig_text    : 11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.\npredict_text : [SOS] In Chapter 11 Jesus called Lazarus from the tomb and raised him from the dead . [EOS]\n-----------------\n"
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    '\\'Bible Coloring\\'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 앱입니다.',\n",
    "    '씨티은행에서 일하세요?', '11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.']\n",
    "\n",
    "for test_sentence in test_sentences:\n",
    "    orig_text = test_sentence\n",
    "    print(orig_text)\n",
    "    test_sentence = src_tokenizer.tokenize(test_sentence)\n",
    "    test_sentence_ids = src_tokenizer.convert_tokens_to_ids(test_sentence)\n",
    "    enc_token = torch.as_tensor([test_sentence_ids], dtype=torch.long)\n",
    "\n",
    "    test_sentence_dec = ['[SOS]']\n",
    "    test_sentence_dec = tgt_tokenizer.convert_tokens_to_ids(test_sentence_dec)\n",
    "    eos_flag = tgt_tokenizer.convert_tokens_to_ids(['[EOS]'])\n",
    "\n",
    "    while test_sentence_dec[-1] != eos_flag[0] or len(test_sentence_dec) > 50:\n",
    "        dec_input = torch.as_tensor([test_sentence_dec], dtype=torch.long)\n",
    "        enc_token, dec_input = enc_token.to(device), dec_input.to(device)\n",
    "        with torch.no_grad():\n",
    "            dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns = transformer(enc_token, dec_input)\n",
    "        predict = torch.argmax(dec_logits, axis=2)[:, -1].squeeze().detach().cpu().numpy()\n",
    "        test_sentence_dec.append(int(predict))\n",
    "\n",
    "    predict_text = ' '.join(tgt_tokenizer.convert_ids_to_tokens(test_sentence_dec))\n",
    "    predict_text = predict_text.replace(\" ##\", \"\")\n",
    "    predict_text = predict_text.replace(\"##\", \"\")\n",
    "    print(f'orig_text    : {orig_text}')\n",
    "    print(f'predict_text : {predict_text}')\n",
    "    print('-----------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}