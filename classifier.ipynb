{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python361064bitpytorchcondaed36d89d6ca44e38823f2dd584949822",
   "display_name": "Python 3.6.10 64-bit ('pytorch': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import json\n",
    "import torch\n",
    "import tokenization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from transformer.model import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'kor_vocab_length': 50000,\n 'eng_vocab_length': 28998,\n 'd_model': 768,\n 'd_ff': 2048,\n 'd_k': 64,\n 'd_v': 64,\n 'num_layers': 12,\n 'num_heads': 8,\n 'start_word': '[SOS]',\n 'end_word': '[EOS]',\n 'sep_word': '[SEP]',\n 'cls_word': '[CLS]',\n 'pad_word': '[PAD]',\n 'mask_word': '[MASK]'}"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read configuration file\n",
    "config = json.load(open('config.json'))\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From /Users/chageumgang/Desktop/transformer_nlp/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n\n"
    },
    {
     "data": {
      "text/plain": "['I', 'love', 'you']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file='vocab/eng_vocab.txt', do_lower_case=False)\n",
    "tokenizer.tokenize('I love you')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "texts : \n[[180, 1633, 9686, 1166, 1124, 1317, 1274, 1124, 190, 1141, 2263, 1108, 1245, 1142, 1109, 172, 1702, 1108, 9383, 1107, 1124, 1200, 1240, 191, 3335, 1305, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [180, 1276, 191, 1633, 6064, 1215, 1130, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [180, 7482, 1633, 1128, 11843, 5932, 2178, 1108, 1840, 1382, 1634, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [180, 1110, 1642, 2298, 11096, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [13282, 1138, 2298, 9180, 2054, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [180, 4821, 1993, 1113, 2298, 178, 27323, 1185, 1166, 1219, 6393, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\nlabels : \n[3, 2, 1, 3, 0, 0]\n"
    }
   ],
   "source": [
    "# define sample dataset\n",
    "dataset = [\n",
    "    ['i feel awful about it too because it s my job to get him in a position to succeed and it just didn t happen here', 'sadness'],\n",
    "    ['i don t feel comfortable around you', 'joy'],\n",
    "    ['i constantly feel an anxious twinge to start something great', 'fear'],\n",
    "    ['i was already feeling drained', 'sadness'],\n",
    "    ['im not feeling bitter today', 'anger'],\n",
    "    ['i hate myself for feeling grumpy about being pregnant', 'anger']]\n",
    "\n",
    "emotion_list = sorted(list(set([data[1] for data in dataset])))\n",
    "\n",
    "token_length = 50\n",
    "\n",
    "texts = []\n",
    "for data in dataset:\n",
    "    token = tokenizer.tokenize(data[0])\n",
    "    while len(token) < token_length:\n",
    "        token.append(config['pad_word'])\n",
    "    texts.append(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "labels = []\n",
    "for data in dataset:\n",
    "    labels.append(emotion_list.index(data[1]))\n",
    "print('texts : ')\n",
    "print(texts)\n",
    "print('labels : ')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure model, optimizer, criterion\n",
    "pad_index = tokenizer.convert_tokens_to_ids([config['pad_word']])[0]\n",
    "classifier = Classifier(\n",
    "        vocab_size=config['eng_vocab_length'],\n",
    "        d_model=config['d_model'],\n",
    "        d_ff=config['d_ff'], d_k=config['d_k'],\n",
    "        d_v=config['d_v'], n_heads=config['num_heads'],\n",
    "        n_layers=config['num_layers'], pad_index=pad_index,\n",
    "        device=device, num_classes=len(emotion_list)).to(device)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=5e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert texts, labels to torch tensor\n",
    "texts = torch.as_tensor(texts, dtype=torch.long).to(device)\n",
    "labels = torch.as_tensor(labels, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0 1.4097980260849\n1 1.6343640089035034\n2 1.1189793348312378\n3 0.18272121250629425\n4 0.14093776047229767\n5 0.12058594077825546\n6 0.1174880638718605\n7 0.038760535418987274\n8 0.008962128311395645\n9 0.0030036389362066984\n"
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits, _ = classifier(texts)\n",
    "    loss = criterion(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "---------\ntext\ni feel awful about it too because it s my job to get him\nemotion\nsadness\n---------\n---------\ntext\ni don t feel comfortable\nemotion\njoy\n---------\n"
    }
   ],
   "source": [
    "test_dataset = [\n",
    "    'i feel awful about it too because it s my job to get him',\n",
    "    'i don t feel comfortable']\n",
    "\n",
    "for test_data in test_dataset:\n",
    "    tokens = tokenizer.tokenize(test_data)\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    tensor = torch.as_tensor([ids], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits, _ = classifier(tensor)\n",
    "    pred = torch.argmax(logits, axis=1)[0]\n",
    "    pred = pred.detach().cpu().numpy()\n",
    "    print('---------')\n",
    "    print('text')\n",
    "    print(test_data)\n",
    "    print('emotion')\n",
    "    print(emotion_list[pred])\n",
    "    print('---------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}